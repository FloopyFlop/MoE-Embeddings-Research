# Project Structure

```
MoE-Embeddings-Research/
‚îÇ
‚îú‚îÄ‚îÄ üìÑ README.md                          # Main documentation
‚îú‚îÄ‚îÄ üìÑ PROJECT_OVERVIEW.md                # Detailed project summary
‚îú‚îÄ‚îÄ üìÑ STRUCTURE.md                       # This file
‚îú‚îÄ‚îÄ üìÑ pyproject.toml                     # Dependencies (managed by uv)
‚îú‚îÄ‚îÄ üìÑ uv.lock                           # Lock file
‚îú‚îÄ‚îÄ üìÑ .gitignore                        # Git ignore rules
‚îú‚îÄ‚îÄ üìÑ .python-version                   # Python version
‚îÇ
‚îú‚îÄ‚îÄ üß™ test_pipeline.py                  # Automated pipeline tests
‚îú‚îÄ‚îÄ üí° example_usage.py                  # Usage examples
‚îÇ
‚îú‚îÄ‚îÄ üìÅ src/                              # Source code (1,804 lines)
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ utils.py                         # Utility functions
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ models/                       # Model architecture
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ encoder.py                   # Transformer encoder
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MultiHeadAttention       # Self-attention mechanism
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FeedForward              # ‚Üê MoE INTEGRATION POINT
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TransformerLayer         # Single transformer layer
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ TransformerEncoder       # Full encoder
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pooling.py                   # Pooling strategies
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Pooler                   # Mean/Max/CLS/MeanMax pooling
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ embeddings.py                # Complete model
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ EmbeddingModel           # Main model class
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ data/                         # Data processing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.py                 # Simple tokenizer
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ SimpleTokenizer          # Word-based tokenization
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dataset.py                   # Dataset classes
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ PairDataset              # For contrastive pairs
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ TripletDataset           # For triplet loss
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ InBatchNegativesDataset  # For in-batch negatives
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ training/                     # Training infrastructure
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ losses.py                    # Loss functions
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ContrastiveLoss          # Classic contrastive
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TripletLoss              # Triplet margin loss
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MultipleNegativesRankingLoss  # InfoNCE/NT-Xent
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ CosineSimilarityLoss     # Direct similarity
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ trainer.py                   # Training loop
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ EmbeddingTrainer         # Complete trainer
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ evaluation/                   # Evaluation metrics
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metrics.py                   # Comprehensive metrics
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ compute_similarity       # Cosine/Euclidean similarity
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ evaluate_retrieval       # P@K, R@K, MRR, MAP
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ evaluate_classification  # k-NN classification
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ evaluate_semantic_similarity  # STS-style evaluation
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ compute_embedding_statistics  # Analysis tools
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ experts/                      # MoE components (FUTURE)
‚îÇ       ‚îî‚îÄ‚îÄ __init__.py                  # Placeholder with roadmap
‚îÇ
‚îú‚îÄ‚îÄ üìÅ notebooks/                        # Jupyter notebooks
‚îÇ   ‚îî‚îÄ‚îÄ demo_training_evaluation.ipynb   # Complete demo
‚îÇ       ‚îú‚îÄ‚îÄ 1. Data Preparation
‚îÇ       ‚îú‚îÄ‚îÄ 2. Tokenizer Building
‚îÇ       ‚îú‚îÄ‚îÄ 3. Model Initialization
‚îÇ       ‚îú‚îÄ‚îÄ 4. Training (20 epochs)
‚îÇ       ‚îú‚îÄ‚îÄ 5. Loss Visualization
‚îÇ       ‚îú‚îÄ‚îÄ 6. Similarity Evaluation
‚îÇ       ‚îú‚îÄ‚îÄ 7. t-SNE Visualization
‚îÇ       ‚îú‚îÄ‚îÄ 8. Heatmaps
‚îÇ       ‚îî‚îÄ‚îÄ 9. Inference Examples
‚îÇ
‚îú‚îÄ‚îÄ üìÅ models/                           # Saved models (created at runtime)
‚îÇ   ‚îî‚îÄ‚îÄ (model checkpoints go here)
‚îÇ
‚îî‚îÄ‚îÄ üìÅ .venv/                            # Virtual environment (managed by uv)
    ‚îî‚îÄ‚îÄ (Python packages)
```

## Component Breakdown

### Core Model Components (src/models/)

```
EmbeddingModel
‚îÇ
‚îú‚îÄ‚îÄ Token Embedding (vocab_size ‚Üí hidden_dim)
‚îú‚îÄ‚îÄ Position Embedding (max_seq_len ‚Üí hidden_dim)
‚îÇ
‚îú‚îÄ‚îÄ TransformerEncoder (6 layers)
‚îÇ   ‚îú‚îÄ‚îÄ Layer 1
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MultiHeadAttention (8 heads)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LayerNorm
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FeedForward (hidden ‚Üí ff_dim ‚Üí hidden) ‚Üê MoE HERE
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ LayerNorm
‚îÇ   ‚îú‚îÄ‚îÄ Layer 2
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ Layer N
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ Pooler (sequence ‚Üí fixed vector)
‚îÇ   ‚îú‚îÄ‚îÄ Mean Pooling (default)
‚îÇ   ‚îú‚îÄ‚îÄ Max Pooling
‚îÇ   ‚îú‚îÄ‚îÄ CLS Pooling
‚îÇ   ‚îî‚îÄ‚îÄ MeanMax Pooling
‚îÇ
‚îî‚îÄ‚îÄ L2 Normalization (for cosine similarity)
```

### Data Flow

```
Input Text
    ‚Üì
[Tokenizer] ‚Üí Token IDs [batch, seq_len]
    ‚Üì
[Embedding] ‚Üí Embedded Tokens [batch, seq_len, hidden_dim]
    ‚Üì
[TransformerEncoder] ‚Üí Contextualized [batch, seq_len, hidden_dim]
    ‚Üì
[Pooler] ‚Üí Sentence Vector [batch, hidden_dim]
    ‚Üì
[L2 Norm] ‚Üí Final Embedding [batch, hidden_dim]
```

### Training Pipeline

```
Dataset (Pairs/Triplets)
    ‚Üì
DataLoader (Batching)
    ‚Üì
Model (Forward Pass)
    ‚Üì
Loss Function (Contrastive/Triplet/InfoNCE)
    ‚Üì
Backpropagation
    ‚Üì
Optimizer (AdamW)
    ‚Üì
Scheduler (Cosine/ReduceLR)
    ‚Üì
Validation
    ‚Üì
Checkpointing
```

## File Sizes & Complexity

| File | Lines | Purpose | Complexity |
|------|-------|---------|-----------|
| `models/encoder.py` | 214 | Transformer architecture | High |
| `models/pooling.py` | 96 | Pooling strategies | Medium |
| `models/embeddings.py` | 143 | Complete model | Medium |
| `data/tokenizer.py` | 140 | Tokenization | Medium |
| `data/dataset.py` | 123 | Dataset classes | Low |
| `training/losses.py` | 139 | Loss functions | Medium |
| `training/trainer.py` | 219 | Training loop | High |
| `evaluation/metrics.py` | 221 | Evaluation metrics | High |
| `utils.py` | 297 | Utility functions | Medium |
| **Total** | **~1,800** | | |

## MoE Integration Roadmap

### Current Architecture
```
FeedForward(x):
    x = Linear(hidden_dim ‚Üí ff_dim)
    x = GELU()
    x = Dropout()
    x = Linear(ff_dim ‚Üí hidden_dim)
    x = Dropout()
    return x
```

### Future MoE Architecture
```
MoEFeedForward(x):
    # Gating
    router_logits = RouterNetwork(x)      # [batch, seq, num_experts]
    expert_weights, expert_indices = TopK(router_logits, k=2)

    # Expert computation
    expert_outputs = []
    for expert_idx in expert_indices:
        expert_output = Expert[expert_idx](x)
        expert_outputs.append(expert_output)

    # Combine
    output = WeightedSum(expert_outputs, expert_weights)

    # Load balancing
    aux_loss = LoadBalancingLoss(router_logits)

    return output, aux_loss
```

## Quick Commands

```bash
# Setup
uv sync

# Test
uv run python test_pipeline.py

# Examples
uv run python example_usage.py

# Jupyter
uv run jupyter notebook notebooks/demo_training_evaluation.ipynb

# Train (custom)
uv run python your_training_script.py
```

## Dependencies Overview

```toml
[project]
name = "moe-embeddings-research"
version = "0.1.0"
requires-python = ">=3.13"

dependencies = [
    "torch",           # Deep learning
    "transformers",    # NLP utilities
    "datasets",        # Dataset management
    "numpy",           # Numerical computing
    "scipy",           # Scientific computing
    "scikit-learn",    # ML utilities
    "matplotlib",      # Plotting
    "seaborn",         # Statistical viz
    "pandas",          # Data manipulation
    "jupyter",         # Notebooks
    "ipykernel",       # Jupyter kernel
    "tqdm",            # Progress bars
]
```

## Next Steps

1. ‚úÖ **Phase 1 Complete**: Base architecture implemented
2. ‚è≥ **Phase 2 Starting**: MoE implementation
   - Implement Expert class
   - Implement Gating mechanism
   - Create MoEFeedForward layer
   - Add load balancing
3. ‚è≥ **Phase 3**: Training & Evaluation
   - Train on diverse domains
   - Analyze expert specialization
   - Compare vs dense baseline
4. ‚è≥ **Phase 4**: Research & Publication
   - Write paper
   - Create visualizations
   - Benchmark results

---

**Status**: ‚úÖ Ready for MoE integration
**Test Coverage**: ‚úÖ All components tested
**Documentation**: ‚úÖ Comprehensive
**Code Quality**: ‚úÖ Production-ready
