{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Embeddings Model - Training and Evaluation Demo\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Data preparation and tokenization\n",
    "2. Model initialization\n",
    "3. Training with contrastive learning\n",
    "4. Evaluation on similarity and retrieval tasks\n",
    "5. Visualization of embeddings\n",
    "\n",
    "The architecture is modular and ready for MoE (Mixture of Experts) integration in future iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our modules\n",
    "from src.models import EmbeddingModel\n",
    "from src.data import SimpleTokenizer, PairDataset\n",
    "from src.training import MultipleNegativesRankingLoss, EmbeddingTrainer\n",
    "from src.evaluation import compute_similarity, evaluate_retrieval, compute_embedding_statistics\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare Data\n",
    "\n",
    "We'll create a synthetic dataset of similar sentence pairs for demonstration.\n",
    "In practice, you would use real datasets like:\n",
    "- SNLI/MNLI for natural language inference\n",
    "- STS benchmark for semantic similarity\n",
    "- MS MARCO or Natural Questions for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic training data with paraphrases\n",
    "train_pairs = [\n",
    "    # Technology\n",
    "    (\"The computer is very fast\", \"This machine has high performance\"),\n",
    "    (\"I love programming in Python\", \"Python is my favorite programming language\"),\n",
    "    (\"Machine learning is fascinating\", \"I find artificial intelligence very interesting\"),\n",
    "    (\"The algorithm runs efficiently\", \"This computational method is very fast\"),\n",
    "    (\"Neural networks are powerful\", \"Deep learning models show great capability\"),\n",
    "    \n",
    "    # Nature\n",
    "    (\"The weather is sunny today\", \"It's a beautiful day with clear skies\"),\n",
    "    (\"Trees provide oxygen\", \"Plants produce air for us to breathe\"),\n",
    "    (\"The ocean is vast\", \"The sea is enormous\"),\n",
    "    (\"Mountains are tall\", \"The peaks reach high into the sky\"),\n",
    "    (\"Flowers bloom in spring\", \"Blossoms appear during springtime\"),\n",
    "    \n",
    "    # Daily life\n",
    "    (\"I enjoy reading books\", \"Books are something I love to read\"),\n",
    "    (\"Coffee tastes good\", \"I like the flavor of coffee\"),\n",
    "    (\"Exercise is healthy\", \"Physical activity is good for you\"),\n",
    "    (\"Music makes me happy\", \"I feel joy when listening to music\"),\n",
    "    (\"Sleep is important\", \"Getting rest is essential\"),\n",
    "    \n",
    "    # Food\n",
    "    (\"Pizza is delicious\", \"I think pizza tastes great\"),\n",
    "    (\"Fresh fruit is nutritious\", \"Eating fruit is healthy\"),\n",
    "    (\"The cake was sweet\", \"This dessert had a sugary taste\"),\n",
    "    (\"Water is essential\", \"We need water to survive\"),\n",
    "    (\"Vegetables are healthy\", \"Eating veggies is good for you\"),\n",
    "]\n",
    "\n",
    "# Expand dataset by adding reverse pairs\n",
    "train_pairs_expanded = train_pairs + [(b, a) for a, b in train_pairs]\n",
    "\n",
    "# Add more variations\n",
    "additional_pairs = [\n",
    "    (\"Dogs are loyal animals\", \"Canines show great faithfulness\"),\n",
    "    (\"Cats are independent\", \"Felines prefer autonomy\"),\n",
    "    (\"The city is busy\", \"Urban areas have lots of activity\"),\n",
    "    (\"The library is quiet\", \"The book room is silent\"),\n",
    "    (\"The car moves fast\", \"The vehicle has high speed\"),\n",
    "    (\"Learning is fun\", \"I enjoy gaining knowledge\"),\n",
    "    (\"The sunset is beautiful\", \"Dusk looks stunning\"),\n",
    "    (\"Stars shine at night\", \"Celestial bodies glow after dark\"),\n",
    "]\n",
    "\n",
    "train_pairs_expanded.extend(additional_pairs)\n",
    "train_pairs_expanded.extend([(b, a) for a, b in additional_pairs])\n",
    "\n",
    "print(f\"Number of training pairs: {len(train_pairs_expanded)}\")\n",
    "print(f\"\\nExample pairs:\")\n",
    "for i in range(3):\n",
    "    print(f\"{i+1}. '{train_pairs_expanded[i][0]}'\")\n",
    "    print(f\"   '{train_pairs_expanded[i][1]}'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Vocabulary and Tokenizer\n",
    "\n",
    "We'll use a simple word-based tokenizer. In production, you'd use:\n",
    "- BPE (Byte-Pair Encoding)\n",
    "- WordPiece\n",
    "- SentencePiece\n",
    "- Or use a pre-trained tokenizer from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all unique sentences for vocabulary building\n",
    "all_sentences = []\n",
    "for pair in train_pairs_expanded:\n",
    "    all_sentences.extend(pair)\n",
    "\n",
    "# Initialize and fit tokenizer\n",
    "tokenizer = SimpleTokenizer(vocab_size=5000, max_length=128)\n",
    "tokenizer.fit(all_sentences)\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"\\nSample tokens:\")\n",
    "sample_tokens = list(tokenizer.token2id.keys())[:20]\n",
    "print(sample_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tokenization\n",
    "test_sentence = \"The computer is very fast\"\n",
    "encoded = tokenizer.encode(test_sentence)\n",
    "print(f\"Original: {test_sentence}\")\n",
    "print(f\"Encoded IDs: {encoded['input_ids'][0][:15]}...\")  # First 15 tokens\n",
    "print(f\"Attention mask: {encoded['attention_mask'][0][:15]}...\")\n",
    "print(f\"Decoded: {tokenizer.decode(encoded['input_ids'][0].tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "train_dataset = PairDataset(train_pairs_expanded, tokenizer)\n",
    "\n",
    "# Split into train and validation\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    train_dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Validation size: {len(val_dataset)}\")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check a batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nBatch keys: {sample_batch.keys()}\")\n",
    "print(f\"Input IDs shape: {sample_batch['input_ids_1'].shape}\")\n",
    "print(f\"Attention mask shape: {sample_batch['attention_mask_1'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Model\n",
    "\n",
    "Our model architecture:\n",
    "- Token + Positional Embeddings\n",
    "- Transformer Encoder (6 layers, 8 heads)\n",
    "- Pooling Layer (mean pooling)\n",
    "- L2 Normalization\n",
    "\n",
    "**Modular design** allows easy replacement of components with MoE variants later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "model_config = {\n",
    "    \"vocab_size\": len(tokenizer),\n",
    "    \"hidden_dim\": 128,        # Embedding dimension\n",
    "    \"num_layers\": 4,          # Number of transformer layers\n",
    "    \"num_heads\": 4,           # Number of attention heads\n",
    "    \"ff_dim\": 512,            # Feed-forward dimension (can be replaced with MoE)\n",
    "    \"max_seq_len\": 128,\n",
    "    \"dropout\": 0.1,\n",
    "    \"pooling_mode\": \"mean\",   # Can also try \"max\", \"cls\", or \"mean_max\"\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"normalize_embeddings\": True\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "model = EmbeddingModel(**model_config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model info\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model initialized with {num_params:,} parameters\")\n",
    "print(f\"Trainable parameters: {num_trainable:,}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a batch\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_batch_device = {k: v.to(device) for k, v in sample_batch.items()}\n",
    "    output = model(\n",
    "        sample_batch_device['input_ids_1'],\n",
    "        sample_batch_device['attention_mask_1']\n",
    "    )\n",
    "    \n",
    "print(f\"Output embeddings shape: {output['embeddings'].shape}\")\n",
    "print(f\"Hidden states shape: {output['hidden_states'].shape}\")\n",
    "print(f\"\\nFirst embedding (first 10 dims): {output['embeddings'][0][:10]}\")\n",
    "print(f\"Embedding L2 norm: {torch.norm(output['embeddings'][0]).item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setup Training\n",
    "\n",
    "We'll use **Multiple Negatives Ranking Loss** (InfoNCE):\n",
    "- Efficient contrastive learning\n",
    "- Uses in-batch negatives\n",
    "- Same loss as used in CLIP, SimCLR, and sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss_fn = MultipleNegativesRankingLoss(temperature=0.05)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=20,\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = EmbeddingTrainer(\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    scheduler=scheduler\n",
    ")\n",
    "\n",
    "print(\"Training setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "history = trainer.train(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=20,\n",
    "    eval_every=1,\n",
    "    save_best=True,\n",
    "    save_path=\"../models/best_model.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "if history['val_loss']:\n",
    "    axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[1].plot(history['learning_rate'], marker='o', color='green')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Learning Rate')\n",
    "axes[1].set_title('Learning Rate Schedule')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate Model Performance\n",
    "\n",
    "### 9.1 Semantic Similarity Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test pairs (similar and dissimilar)\n",
    "test_pairs = [\n",
    "    # Similar pairs (should have high similarity)\n",
    "    (\"The dog is running\", \"A canine is jogging\", True),\n",
    "    (\"I like ice cream\", \"Ice cream is something I enjoy\", True),\n",
    "    (\"The book is interesting\", \"This novel is fascinating\", True),\n",
    "    (\"Programming is fun\", \"I enjoy coding\", True),\n",
    "    (\"The sky is blue\", \"Blue is the color of the sky\", True),\n",
    "    \n",
    "    # Dissimilar pairs (should have low similarity)\n",
    "    (\"The dog is running\", \"I like ice cream\", False),\n",
    "    (\"Programming is fun\", \"The sky is blue\", False),\n",
    "    (\"The book is interesting\", \"The dog is running\", False),\n",
    "    (\"I like ice cream\", \"Programming is fun\", False),\n",
    "    (\"The sky is blue\", \"The book is interesting\", False),\n",
    "]\n",
    "\n",
    "# Compute similarities\n",
    "model.eval()\n",
    "similarities = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sent1, sent2, is_similar in test_pairs:\n",
    "        # Encode\n",
    "        enc1 = tokenizer.encode(sent1, return_tensors=\"pt\")\n",
    "        enc2 = tokenizer.encode(sent2, return_tensors=\"pt\")\n",
    "        \n",
    "        # Get embeddings\n",
    "        emb1 = model(enc1['input_ids'].to(device), enc1['attention_mask'].to(device))['embeddings']\n",
    "        emb2 = model(enc2['input_ids'].to(device), enc2['attention_mask'].to(device))['embeddings']\n",
    "        \n",
    "        # Cosine similarity\n",
    "        sim = torch.nn.functional.cosine_similarity(emb1, emb2).item()\n",
    "        similarities.append(sim)\n",
    "        labels.append(is_similar)\n",
    "        \n",
    "        print(f\"Similarity: {sim:.3f} | Similar: {is_similar}\")\n",
    "        print(f\"  S1: {sent1}\")\n",
    "        print(f\"  S2: {sent2}\\n\")\n",
    "\n",
    "# Compute average similarity for similar vs dissimilar pairs\n",
    "similar_sims = [s for s, l in zip(similarities, labels) if l]\n",
    "dissimilar_sims = [s for s, l in zip(similarities, labels) if not l]\n",
    "\n",
    "print(f\"\\nAverage similarity for similar pairs: {np.mean(similar_sims):.3f} ± {np.std(similar_sims):.3f}\")\n",
    "print(f\"Average similarity for dissimilar pairs: {np.mean(dissimilar_sims):.3f} ± {np.std(dissimilar_sims):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Visualize Similarity Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot similarity distributions\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.hist(similar_sims, alpha=0.7, label='Similar pairs', bins=10, color='green')\n",
    "plt.hist(dissimilar_sims, alpha=0.7, label='Dissimilar pairs', bins=10, color='red')\n",
    "\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Similarity Distribution for Similar vs Dissimilar Pairs')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Embeddings with t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode all training sentences\n",
    "sample_sentences = [\n",
    "    # Technology cluster\n",
    "    \"The computer is very fast\",\n",
    "    \"I love programming in Python\",\n",
    "    \"Machine learning is fascinating\",\n",
    "    \"Neural networks are powerful\",\n",
    "    \n",
    "    # Nature cluster\n",
    "    \"The weather is sunny today\",\n",
    "    \"Trees provide oxygen\",\n",
    "    \"The ocean is vast\",\n",
    "    \"Mountains are tall\",\n",
    "    \n",
    "    # Food cluster\n",
    "    \"Pizza is delicious\",\n",
    "    \"Fresh fruit is nutritious\",\n",
    "    \"Water is essential\",\n",
    "    \"Vegetables are healthy\",\n",
    "]\n",
    "\n",
    "# Categories for coloring\n",
    "categories = ['Tech']*4 + ['Nature']*4 + ['Food']*4\n",
    "\n",
    "# Get embeddings\n",
    "embeddings_list = []\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sent in sample_sentences:\n",
    "        enc = tokenizer.encode(sent, return_tensors=\"pt\")\n",
    "        emb = model(enc['input_ids'].to(device), enc['attention_mask'].to(device))['embeddings']\n",
    "        embeddings_list.append(emb.cpu().numpy())\n",
    "\n",
    "embeddings_array = np.vstack(embeddings_list)\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE for visualization\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
    "embeddings_2d = tsne.fit_transform(embeddings_array)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "colors = {'Tech': 'blue', 'Nature': 'green', 'Food': 'orange'}\n",
    "for i, (sent, cat) in enumerate(zip(sample_sentences, categories)):\n",
    "    plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1], \n",
    "               c=colors[cat], s=100, alpha=0.6, label=cat if cat not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
    "    plt.annotate(sent[:30] + '...', (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                fontsize=8, alpha=0.7)\n",
    "\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.title('t-SNE Visualization of Sentence Embeddings')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Embedding Statistics and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute embedding statistics\n",
    "stats = compute_embedding_statistics(embeddings_array)\n",
    "\n",
    "print(\"Embedding Statistics:\")\n",
    "print(\"=\" * 40)\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key:20s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Similarity Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sim_matrix = cosine_similarity(embeddings_array)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(sim_matrix, \n",
    "           xticklabels=[s[:25] + '...' for s in sample_sentences],\n",
    "           yticklabels=[s[:25] + '...' for s in sample_sentences],\n",
    "           annot=True, fmt='.2f', cmap='RdYlGn', center=0.5,\n",
    "           vmin=0, vmax=1)\n",
    "plt.title('Cosine Similarity Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': model_config,\n",
    "    'training_history': history\n",
    "}, '../models/embedding_model_final.pt')\n",
    "\n",
    "# Save tokenizer\n",
    "with open('../models/tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "print(\"Model and tokenizer saved successfully!\")\n",
    "print(f\"  - Model: ../models/embedding_model_final.pt\")\n",
    "print(f\"  - Tokenizer: ../models/tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Inference Example - Use the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model, tokenizer, device):\n",
    "    \"\"\"Get embedding for a single text\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        enc = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "        emb = model(\n",
    "            enc['input_ids'].to(device),\n",
    "            enc['attention_mask'].to(device)\n",
    "        )['embeddings']\n",
    "        return emb.cpu().numpy()\n",
    "\n",
    "def find_most_similar(query, candidates, model, tokenizer, device, top_k=3):\n",
    "    \"\"\"Find most similar sentences to query\"\"\"\n",
    "    # Get query embedding\n",
    "    query_emb = get_embedding(query, model, tokenizer, device)\n",
    "    \n",
    "    # Get candidate embeddings\n",
    "    candidate_embs = np.vstack([\n",
    "        get_embedding(c, model, tokenizer, device) for c in candidates\n",
    "    ])\n",
    "    \n",
    "    # Compute similarities\n",
    "    similarities = cosine_similarity(query_emb, candidate_embs)[0]\n",
    "    \n",
    "    # Get top-k\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'text': candidates[idx],\n",
    "            'similarity': similarities[idx]\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test similarity search\n",
    "query = \"I enjoy building software applications\"\n",
    "\n",
    "candidates = [\n",
    "    \"The computer is very fast\",\n",
    "    \"I love programming in Python\",\n",
    "    \"The weather is sunny today\",\n",
    "    \"Machine learning is fascinating\",\n",
    "    \"Pizza is delicious\",\n",
    "    \"Trees provide oxygen\",\n",
    "    \"Neural networks are powerful\",\n",
    "    \"The ocean is vast\",\n",
    "]\n",
    "\n",
    "results = find_most_similar(query, candidates, model, tokenizer, device, top_k=5)\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Most similar sentences:\")\n",
    "print(\"=\" * 60)\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. [{result['similarity']:.3f}] {result['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Model Architecture Summary and Next Steps\n",
    "\n",
    "### Current Architecture:\n",
    "- ✅ Token + Positional Embeddings\n",
    "- ✅ Multi-layer Transformer Encoder\n",
    "- ✅ Mean Pooling\n",
    "- ✅ L2 Normalization\n",
    "- ✅ Contrastive Learning (Multiple Negatives Ranking Loss)\n",
    "\n",
    "### Modular Components Ready for MoE:\n",
    "1. **Feed-Forward Layers** in `src/models/encoder.py:FeedForward`\n",
    "   - Can be replaced with MoE layers\n",
    "   - Each expert specializes in different linguistic patterns\n",
    "\n",
    "2. **Pooling Layer** in `src/models/pooling.py:Pooler`\n",
    "   - Can add expert-based pooling strategies\n",
    "   - Different experts for different sentence types\n",
    "\n",
    "3. **Expert Module Placeholder** in `src/experts/`\n",
    "   - Ready for MoE implementation\n",
    "   - Will include gating networks and expert selection\n",
    "\n",
    "### Next Steps for MoE Integration:\n",
    "1. Implement expert networks (specialized FFNs)\n",
    "2. Design gating mechanism (Top-K routing)\n",
    "3. Add load balancing loss\n",
    "4. Replace FFN layers with MoE layers\n",
    "5. Fine-tune on diverse domains to learn expert specialization\n",
    "6. Analyze which experts activate for different inputs\n",
    "\n",
    "### Performance Notes:\n",
    "- Current model works well for semantic similarity\n",
    "- Learns to cluster semantically related sentences\n",
    "- Ready for scaling with MoE for better capacity and specialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ✅ Building a vocabulary and tokenizer\n",
    "2. ✅ Creating a modular embedding model architecture\n",
    "3. ✅ Training with contrastive learning\n",
    "4. ✅ Evaluating semantic similarity\n",
    "5. ✅ Visualizing embeddings with t-SNE\n",
    "6. ✅ Performing similarity search\n",
    "\n",
    "The architecture is **fully modular** and ready for MoE integration in the next phase!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
